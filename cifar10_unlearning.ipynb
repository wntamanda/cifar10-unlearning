{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9345e11d-963e-4525-9828-1a80940f0710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Config & Repro\n",
    "import os, random, json\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "with open(\"config.json\") as f:\n",
    "    C = json.load(f)\n",
    "\n",
    "SEED = C[\"seed\"]\n",
    "BATCH_SIZE = C[\"batch_size\"]\n",
    "LR = C[\"lr\"]\n",
    "EPOCHS = C[\"epochs\"]\n",
    "\n",
    "# CIFAR-10 ids: 0 airplane, 1 automobile, 2 bird, 3 cat, 4 deer, 5 dog, 6 frog, 7 horse, 8 ship, 9 truck\n",
    "BASE_CLASSES = C[\"base_classes\"] # bird, cat, dog, truck\n",
    "FORGET_CLASS = C[\"forget_class\"] # airplane\n",
    "PER_BASE = C[\"per_base\"] # ~1000 per base class\n",
    "FORGET_N = C[\"forget_n\"] # ~1000 airplanes\n",
    "\n",
    "def set_seed(seed=SEED):\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "# Torch generator for DataLoader shuffles\n",
    "g = torch.Generator()\n",
    "g.manual_seed(SEED)\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fcd5572-89da-47c1-9808-e40b18612110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['airplane',\n",
       " 'automobile',\n",
       " 'bird',\n",
       " 'cat',\n",
       " 'deer',\n",
       " 'dog',\n",
       " 'frog',\n",
       " 'horse',\n",
       " 'ship',\n",
       " 'truck']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Core imports\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Transforms (keep fixed across phases)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Load CIFAR-10\n",
    "train_dataset = torchvision.datasets.CIFAR10(root=\"./data\", train=True,  download=True, transform=transform)\n",
    "test_dataset  = torchvision.datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
    "\n",
    "class_names = train_dataset.classes\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "140eee09-af2e-4d2e-912c-8374eddca148",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 1000, 5000, 1000, 9000)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from torch.utils.data import Subset, ConcatDataset, DataLoader\n",
    "\n",
    "# Build class->indices map for train\n",
    "cls_to_idxs = defaultdict(list)\n",
    "for i, (_, y) in enumerate(train_dataset):\n",
    "    cls_to_idxs[int(y)].append(i)\n",
    "\n",
    "# Deterministic shuffle\n",
    "rng = random.Random(SEED)\n",
    "for c in cls_to_idxs:\n",
    "    rng.shuffle(cls_to_idxs[c])\n",
    "\n",
    "# Select fixed indices\n",
    "base_indices = []\n",
    "for c in BASE_CLASSES:\n",
    "    base_indices += cls_to_idxs[c][:PER_BASE]\n",
    "forget_indices = cls_to_idxs[FORGET_CLASS][:FORGET_N]\n",
    "\n",
    "# Save splits (so they’re frozen across runs)\n",
    "splits = {\n",
    "    \"seed\": SEED,\n",
    "    \"base_classes\": BASE_CLASSES,\n",
    "    \"forget_class\": FORGET_CLASS,\n",
    "    \"base_indices\": sorted(base_indices),\n",
    "    \"forget_indices\": sorted(forget_indices),\n",
    "}\n",
    "with open(\"splits_train.json\", \"w\") as f:\n",
    "    json.dump(splits, f)\n",
    "\n",
    "# Build train subsets\n",
    "dataset_base   = Subset(train_dataset, splits[\"base_indices\"])\n",
    "dataset_forget = Subset(train_dataset, splits[\"forget_indices\"])\n",
    "dataset_full   = ConcatDataset([dataset_base, dataset_forget])  # base + airplane\n",
    "\n",
    "# Build fixed test subsets\n",
    "test_forget_indices = [i for i, (_, y) in enumerate(test_dataset) if int(y) == FORGET_CLASS]\n",
    "test_retain_indices = [i for i, (_, y) in enumerate(test_dataset) if int(y) != FORGET_CLASS]\n",
    "\n",
    "with open(\"splits_test.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"forget_test_indices\": test_forget_indices,\n",
    "        \"retain_test_indices\": test_retain_indices\n",
    "    }, f)\n",
    "\n",
    "test_forget_ds = Subset(test_dataset, test_forget_indices)\n",
    "test_retain_ds = Subset(test_dataset, test_retain_indices)\n",
    "\n",
    "# Deterministic DataLoaders (CPU -> num_workers=0)\n",
    "loader_base   = DataLoader(dataset_base,   batch_size=BATCH_SIZE, shuffle=True,\n",
    "                           num_workers=0, worker_init_fn=seed_worker, generator=g)\n",
    "loader_forget = DataLoader(dataset_forget, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                           num_workers=0, worker_init_fn=seed_worker, generator=g)\n",
    "loader_full   = DataLoader(dataset_full,   batch_size=BATCH_SIZE, shuffle=True,\n",
    "                           num_workers=0, worker_init_fn=seed_worker, generator=g)\n",
    "\n",
    "loader_test_overall = DataLoader(test_dataset,   batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "loader_test_forget  = DataLoader(test_forget_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "loader_test_retain  = DataLoader(test_retain_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "len(dataset_base), len(dataset_forget), len(dataset_full), len(test_forget_ds), len(test_retain_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca7056e8-3a48-4ab4-b593-0713c3849847",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)  # 32→16→8→4\n",
    "        self.fc1 = nn.Linear(256 * 2 * 2, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # [32,16,16]\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # [64,8,8]\n",
    "        x = self.pool(F.relu(self.conv3(x)))  # [128,4,4]\n",
    "        x = self.pool(F.relu(self.conv4(x)))  # [256,2,2]\n",
    "        x = x.view(-1, 256 * 2 * 2)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model_base    = SimpleCNN(num_classes=10).to(device)\n",
    "model_full    = SimpleCNN(num_classes=10).to(device)\n",
    "model_retrain = SimpleCNN(num_classes=10).to(device)\n",
    "\n",
    "opt_base    = optim.Adam(model_base.parameters(),    lr=LR)\n",
    "opt_full    = optim.Adam(model_full.parameters(),    lr=LR)\n",
    "opt_retrain = optim.Adam(model_retrain.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2617ef80-2366-43d6-b3ab-dd78b1035299",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device, criterion):\n",
    "    model.eval()\n",
    "    loss_sum, correct, total = 0.0, 0, 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss_sum += loss.item() * y.size(0)\n",
    "        pred = logits.argmax(1)\n",
    "        correct += (pred == y).sum().item()\n",
    "        total += y.size(0)\n",
    "    return {\"loss\": loss_sum / max(1, total), \"acc\": correct / max(1, total)}\n",
    "\n",
    "def train_model(model, dataloader, optimizer, criterion, device, num_epochs=EPOCHS, phase_name=\"\"):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss, correct, total = 0.0, 0, 0\n",
    "        for inputs, labels in dataloader:\n",
    "            # labels stay as original CIFAR-10 ids (0..9)\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            running_loss += loss.item() * labels.size(0)\n",
    "\n",
    "        train_loss = running_loss / max(1, total)\n",
    "        train_acc  = correct / max(1, total)\n",
    "        print(f\"[{phase_name}] Epoch {epoch+1}: Loss={train_loss:.3f} | Acc={100*train_acc:.2f}%\")\n",
    "\n",
    "    print(f\"{phase_name} training complete\")\n",
    "\n",
    "def report_all(name, model):\n",
    "    res_overall = evaluate(model, loader_test_overall, device, criterion)\n",
    "    res_forget  = evaluate(model, loader_test_forget,  device, criterion)\n",
    "    res_retain  = evaluate(model, loader_test_retain,  device, criterion)\n",
    "    print(f\"\\n== {name} Test ==\")\n",
    "    print(f\"Overall: acc={100*res_overall['acc']:.2f}%, loss={res_overall['loss']:.3f}\")\n",
    "    print(f\"Forget (airplane): acc={100*res_forget['acc']:.2f}%, loss={res_forget['loss']:.3f}\")\n",
    "    print(f\"Retain (non-airplane): acc={100*res_retain['acc']:.2f}%, loss={res_retain['loss']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b17afcc-4289-4788-b93f-77f3f0244f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unlearning metrics: capture + distance/score\n",
    "import math\n",
    "\n",
    "# Registry to hold results per model name\n",
    "RUN_RESULTS = {}  # e.g. {\"BASE\": {...}, \"FULL\": {...}}\n",
    "\n",
    "def eval_and_store(name, model):\n",
    "    \"\"\"Evaluate on overall / forget / retain and store in RUN_RESULTS[name].\"\"\"\n",
    "    overall = evaluate(model, loader_test_overall, device, criterion)\n",
    "    forget  = evaluate(model, loader_test_forget,  device, criterion)\n",
    "    retain  = evaluate(model, loader_test_retain,  device, criterion)\n",
    "    RUN_RESULTS[name] = {\n",
    "        \"overall\": overall,\n",
    "        \"forget\":  forget,\n",
    "        \"retain\":  retain\n",
    "    }\n",
    "    print(f\"\\n== {name} Test ==\")\n",
    "    print(f\"Overall: acc={100*overall['acc']:.2f}%, loss={overall['loss']:.3f}\")\n",
    "    print(f\"Forget (airplane): acc={100*forget['acc']:.2f}%, loss={forget['loss']:.3f}\")\n",
    "    print(f\"Retain (non-airplane): acc={100*retain['acc']:.2f}%, loss={retain['loss']:.3f}\")\n",
    "    return RUN_RESULTS[name]\n",
    "\n",
    "def unlearning_distance(candidate_name, full_name=\"FULL\", retrain_name=\"RETRAIN_BASE\", alpha=0.5):\n",
    "    \"\"\"\n",
    "    Compute gaps for an unlearned model vs baselines.\n",
    "    - We want candidate_forget_acc ~ retrain_forget_acc (close to retrain)\n",
    "    - We want candidate_retain_acc ~ full_retain_acc   (close to full)\n",
    "    alpha weighs the forget gap; (1-alpha) weighs the retain gap. Default 0.5/0.5.\n",
    "    Score in [0,1]: higher is better.\n",
    "    \"\"\"\n",
    "    assert candidate_name in RUN_RESULTS, \"Candidate not evaluated yet.\"\n",
    "    assert full_name in RUN_RESULTS and retrain_name in RUN_RESULTS, \"Run FULL and RETRAIN_BASE first (and eval/store).\"\n",
    "    cand   = RUN_RESULTS[candidate_name]\n",
    "    full   = RUN_RESULTS[full_name]\n",
    "    retr   = RUN_RESULTS[retrain_name]\n",
    "\n",
    "    # Accuracies in [0,1]\n",
    "    acc_c_forget = cand[\"forget\"][\"acc\"]\n",
    "    acc_c_retain = cand[\"retain\"][\"acc\"]\n",
    "    acc_f_retain = full[\"retain\"][\"acc\"]\n",
    "    acc_r_forget = retr[\"forget\"][\"acc\"]\n",
    "\n",
    "    # Absolute gaps\n",
    "    forget_gap = abs(acc_c_forget - acc_r_forget)   # want -> 0\n",
    "    retain_gap = abs(acc_c_retain - acc_f_retain)   # want -> 0\n",
    "\n",
    "    # Combined score (normalize by 1 since acc ∈ [0,1])\n",
    "    score = 1.0 - (alpha * forget_gap + (1.0 - alpha) * retain_gap)\n",
    "\n",
    "    return {\n",
    "        \"candidate\": candidate_name,\n",
    "        \"reference_full\": full_name,\n",
    "        \"reference_retrain\": retrain_name,\n",
    "        \"forget_gap\": forget_gap,\n",
    "        \"retain_gap\": retain_gap,\n",
    "        \"alpha\": alpha,\n",
    "        \"score\": max(0.0, min(1.0, score))  # clamp just in case\n",
    "    }\n",
    "\n",
    "def print_unlearning_distance(stats):\n",
    "    print(f\"\\n== Unlearning Distance: {stats['candidate']} ==\")\n",
    "    print(f\"forget_gap (→ retrain): {stats['forget_gap']:.4f}\")\n",
    "    print(f\"retain_gap (→ full)   : {stats['retain_gap']:.4f}\")\n",
    "    print(f\"alpha (forget weight) : {stats['alpha']:.2f}\")\n",
    "    print(f\"UNLEARNING SCORE      : {stats['score']:.4f}  (1.0 is best)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65ebb7c1-3ee1-4647-b989-c276724ec5a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BASE] Epoch 1: Loss=1.327 | Acc=38.17%\n",
      "[BASE] Epoch 2: Loss=1.010 | Acc=53.40%\n",
      "[BASE] Epoch 3: Loss=0.873 | Acc=61.70%\n",
      "[BASE] Epoch 4: Loss=0.797 | Acc=66.07%\n",
      "[BASE] Epoch 5: Loss=0.746 | Acc=68.65%\n",
      "BASE training complete\n",
      "\n",
      "== BASE Test ==\n",
      "Overall: acc=25.33%, loss=6.749\n",
      "Forget (airplane): acc=0.00%, loss=10.990\n",
      "Retain (non-airplane): acc=28.14%, loss=6.278\n",
      "[FULL] Epoch 1: Loss=1.455 | Acc=34.96%\n",
      "[FULL] Epoch 2: Loss=1.163 | Acc=49.64%\n",
      "[FULL] Epoch 3: Loss=1.008 | Acc=57.58%\n",
      "[FULL] Epoch 4: Loss=0.923 | Acc=61.48%\n",
      "[FULL] Epoch 5: Loss=0.862 | Acc=64.44%\n",
      "FULL training complete\n",
      "\n",
      "== FULL Test ==\n",
      "Overall: acc=31.70%, loss=6.494\n",
      "Forget (airplane): acc=74.70%, loss=0.669\n",
      "Retain (non-airplane): acc=26.92%, loss=7.141\n",
      "[RETRAIN_BASE] Epoch 1: Loss=1.383 | Acc=33.85%\n",
      "[RETRAIN_BASE] Epoch 2: Loss=1.043 | Acc=50.62%\n",
      "[RETRAIN_BASE] Epoch 3: Loss=0.881 | Acc=61.20%\n",
      "[RETRAIN_BASE] Epoch 4: Loss=0.839 | Acc=62.90%\n",
      "[RETRAIN_BASE] Epoch 5: Loss=0.776 | Acc=67.12%\n",
      "RETRAIN_BASE training complete\n",
      "\n",
      "== RETRAIN_BASE Test ==\n",
      "Overall: acc=26.60%, loss=8.014\n",
      "Forget (airplane): acc=0.00%, loss=13.118\n",
      "Retain (non-airplane): acc=29.56%, loss=7.447\n"
     ]
    }
   ],
   "source": [
    "# Phase A: BASE (train on base classes only)\n",
    "train_model(model_base, loader_base, opt_base, criterion, device, num_epochs=EPOCHS, phase_name=\"BASE\")\n",
    "torch.save({\n",
    "    \"model_state\": model_base.state_dict(),\n",
    "    \"optimizer_state\": opt_base.state_dict(),\n",
    "    \"config\": {\"seed\": SEED, \"lr\": LR, \"epochs\": EPOCHS, \"phase\": \"BASE\",\n",
    "               \"base_classes\": BASE_CLASSES, \"forget_class\": FORGET_CLASS,\n",
    "               \"per_base\": PER_BASE, \"forget_n\": FORGET_N}\n",
    "}, \"model_base.pt\")\n",
    "report_all(\"BASE\", model_base)\n",
    "\n",
    "# Phase B: FULL (add airplane)\n",
    "train_model(model_full, loader_full, opt_full, criterion, device, num_epochs=EPOCHS, phase_name=\"FULL\")\n",
    "torch.save({\n",
    "    \"model_state\": model_full.state_dict(),\n",
    "    \"optimizer_state\": opt_full.state_dict(),\n",
    "    \"config\": {\"seed\": SEED, \"lr\": LR, \"epochs\": EPOCHS, \"phase\": \"FULL\"}\n",
    "}, \"model_full.pt\")\n",
    "report_all(\"FULL\", model_full)\n",
    "\n",
    "# Phase C: RETRAIN_BASE (scratch baseline without airplane)\n",
    "train_model(model_retrain, loader_base, opt_retrain, criterion, device, num_epochs=EPOCHS, phase_name=\"RETRAIN_BASE\")\n",
    "torch.save({\n",
    "    \"model_state\": model_retrain.state_dict(),\n",
    "    \"optimizer_state\": opt_retrain.state_dict(),\n",
    "    \"config\": {\"seed\": SEED, \"lr\": LR, \"epochs\": EPOCHS, \"phase\": \"RETRAIN_BASE\"}\n",
    "}, \"model_retrain.pt\")\n",
    "report_all(\"RETRAIN_BASE\", model_retrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8dc1f07e-e3f2-432b-b3b4-e07afa08f00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_and_store(\"BASE\",         model_base)\n",
    "# eval_and_store(\"FULL\",         model_full)\n",
    "# eval_and_store(\"RETRAIN_BASE\", model_retrain)\n",
    "\n",
    "# eval_and_store(\"UNLEARNED\", model_unlearn)\n",
    "\n",
    "# stats = unlearning_distance(\"UNLEARNED\", full_name=\"FULL\", retrain_name=\"RETRAIN_BASE\", alpha=0.5)\n",
    "# print_unlearning_distance(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0323b105-8fdd-4617-b993-b36c427154a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (unlearn-env)",
   "language": "python",
   "name": "unlearn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
