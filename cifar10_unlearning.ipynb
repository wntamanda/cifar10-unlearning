{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9345e11d-963e-4525-9828-1a80940f0710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config & Repro\n",
    "import os, random, json\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "with open(\"config.json\") as f:\n",
    "    C = json.load(f)\n",
    "\n",
    "SEED = C[\"seed\"]\n",
    "BATCH_SIZE = C[\"batch_size\"]\n",
    "LR = C[\"lr\"]\n",
    "EPOCHS = C[\"epochs\"]\n",
    "\n",
    "# CIFAR-10 ids: 0 airplane, 1 automobile, 2 bird, 3 cat, 4 deer, 5 dog, 6 frog, 7 horse, 8 ship, 9 truck\n",
    "BASE_CLASSES = C[\"base_classes\"] # bird, cat, dog, truck\n",
    "FORGET_CLASS = C[\"forget_class\"] # airplane\n",
    "PER_BASE = C[\"per_base\"] # ~1000 per base class\n",
    "FORGET_N = C[\"forget_n\"] # ~1000 airplanes\n",
    "\n",
    "def set_seed(seed=SEED):\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "# Torch generator for DataLoader shuffles\n",
    "g = torch.Generator()\n",
    "g.manual_seed(SEED)\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcd5572-89da-47c1-9808-e40b18612110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Transforms (keep fixed across phases)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Load CIFAR-10\n",
    "train_dataset = torchvision.datasets.CIFAR10(root=\"./data\", train=True,  download=True, transform=transform)\n",
    "test_dataset  = torchvision.datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
    "\n",
    "class_names = train_dataset.classes\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140eee09-af2e-4d2e-912c-8374eddca148",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from torch.utils.data import Subset, ConcatDataset, DataLoader\n",
    "\n",
    "# Build class->indices map for train\n",
    "cls_to_idxs = defaultdict(list)\n",
    "for i, (_, y) in enumerate(train_dataset):\n",
    "    cls_to_idxs[int(y)].append(i)\n",
    "\n",
    "# Deterministic shuffle\n",
    "rng = random.Random(SEED)\n",
    "for c in cls_to_idxs:\n",
    "    rng.shuffle(cls_to_idxs[c])\n",
    "\n",
    "# Select fixed indices\n",
    "base_indices = []\n",
    "for c in BASE_CLASSES:\n",
    "    base_indices += cls_to_idxs[c][:PER_BASE]\n",
    "forget_indices = cls_to_idxs[FORGET_CLASS][:FORGET_N]\n",
    "\n",
    "# Save splits (so they’re frozen across runs)\n",
    "splits = {\n",
    "    \"seed\": SEED,\n",
    "    \"base_classes\": BASE_CLASSES,\n",
    "    \"forget_class\": FORGET_CLASS,\n",
    "    \"base_indices\": sorted(base_indices),\n",
    "    \"forget_indices\": sorted(forget_indices),\n",
    "}\n",
    "with open(\"splits_train.json\", \"w\") as f:\n",
    "    json.dump(splits, f)\n",
    "\n",
    "# Build train subsets\n",
    "dataset_base   = Subset(train_dataset, splits[\"base_indices\"])\n",
    "dataset_forget = Subset(train_dataset, splits[\"forget_indices\"])\n",
    "dataset_full   = ConcatDataset([dataset_base, dataset_forget])  # base + airplane\n",
    "\n",
    "# Build fixed test subsets\n",
    "test_forget_indices = [i for i, (_, y) in enumerate(test_dataset) if int(y) == FORGET_CLASS]\n",
    "test_retain_indices = [i for i, (_, y) in enumerate(test_dataset) if int(y) != FORGET_CLASS]\n",
    "\n",
    "with open(\"splits_test.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"forget_test_indices\": test_forget_indices,\n",
    "        \"retain_test_indices\": test_retain_indices\n",
    "    }, f)\n",
    "\n",
    "test_forget_ds = Subset(test_dataset, test_forget_indices)\n",
    "test_retain_ds = Subset(test_dataset, test_retain_indices)\n",
    "\n",
    "# Deterministic DataLoaders (CPU -> num_workers=0)\n",
    "loader_base   = DataLoader(dataset_base,   batch_size=BATCH_SIZE, shuffle=True,\n",
    "                           num_workers=0, worker_init_fn=seed_worker, generator=g)\n",
    "loader_forget = DataLoader(dataset_forget, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                           num_workers=0, worker_init_fn=seed_worker, generator=g)\n",
    "loader_full   = DataLoader(dataset_full,   batch_size=BATCH_SIZE, shuffle=True,\n",
    "                           num_workers=0, worker_init_fn=seed_worker, generator=g)\n",
    "\n",
    "loader_test_overall = DataLoader(test_dataset,   batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "loader_test_forget  = DataLoader(test_forget_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "loader_test_retain  = DataLoader(test_retain_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "len(dataset_base), len(dataset_forget), len(dataset_full), len(test_forget_ds), len(test_retain_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7056e8-3a48-4ab4-b593-0713c3849847",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)  # 32→16→8→4\n",
    "        self.fc1 = nn.Linear(256 * 2 * 2, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # [32,16,16]\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # [64,8,8]\n",
    "        x = self.pool(F.relu(self.conv3(x)))  # [128,4,4]\n",
    "        x = self.pool(F.relu(self.conv4(x)))  # [256,2,2]\n",
    "        x = x.view(-1, 256 * 2 * 2)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model_base    = SimpleCNN(num_classes=10).to(device)\n",
    "model_full    = SimpleCNN(num_classes=10).to(device)\n",
    "model_retrain = SimpleCNN(num_classes=10).to(device)\n",
    "\n",
    "opt_base    = optim.Adam(model_base.parameters(),    lr=LR)\n",
    "opt_full    = optim.Adam(model_full.parameters(),    lr=LR)\n",
    "opt_retrain = optim.Adam(model_retrain.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2617ef80-2366-43d6-b3ab-dd78b1035299",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device, criterion):\n",
    "    model.eval()\n",
    "    loss_sum, correct, total = 0.0, 0, 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss_sum += loss.item() * y.size(0)\n",
    "        pred = logits.argmax(1)\n",
    "        correct += (pred == y).sum().item()\n",
    "        total += y.size(0)\n",
    "    return {\"loss\": loss_sum / max(1, total), \"acc\": correct / max(1, total)}\n",
    "\n",
    "def train_model(model, dataloader, optimizer, criterion, device, num_epochs=EPOCHS, phase_name=\"\"):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss, correct, total = 0.0, 0, 0\n",
    "        for inputs, labels in dataloader:\n",
    "            # labels stay as original CIFAR-10 ids (0..9)\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            running_loss += loss.item() * labels.size(0)\n",
    "\n",
    "        train_loss = running_loss / max(1, total)\n",
    "        train_acc  = correct / max(1, total)\n",
    "        print(f\"[{phase_name}] Epoch {epoch+1}: Loss={train_loss:.3f} | Acc={100*train_acc:.2f}%\")\n",
    "\n",
    "    print(f\"{phase_name} training complete\")\n",
    "\n",
    "def report_all(name, model):\n",
    "    res_overall = evaluate(model, loader_test_overall, device, criterion)\n",
    "    res_forget  = evaluate(model, loader_test_forget,  device, criterion)\n",
    "    res_retain  = evaluate(model, loader_test_retain,  device, criterion)\n",
    "    print(f\"\\n== {name} Test ==\")\n",
    "    print(f\"Overall: acc={100*res_overall['acc']:.2f}%, loss={res_overall['loss']:.3f}\")\n",
    "    print(f\"Forget (airplane): acc={100*res_forget['acc']:.2f}%, loss={res_forget['loss']:.3f}\")\n",
    "    print(f\"Retain (non-airplane): acc={100*res_retain['acc']:.2f}%, loss={res_retain['loss']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b17afcc-4289-4788-b93f-77f3f0244f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unlearning metrics: capture + distance/score\n",
    "import math\n",
    "\n",
    "# Registry to hold results per model name\n",
    "RUN_RESULTS = {}  # e.g. {\"BASE\": {...}, \"FULL\": {...}}\n",
    "\n",
    "def eval_and_store(name, model):\n",
    "    \"\"\"Evaluate on overall / forget / retain and store in RUN_RESULTS[name].\"\"\"\n",
    "    overall = evaluate(model, loader_test_overall, device, criterion)\n",
    "    forget  = evaluate(model, loader_test_forget,  device, criterion)\n",
    "    retain  = evaluate(model, loader_test_retain,  device, criterion)\n",
    "    RUN_RESULTS[name] = {\n",
    "        \"overall\": overall,\n",
    "        \"forget\":  forget,\n",
    "        \"retain\":  retain\n",
    "    }\n",
    "    print(f\"\\n== {name} Test ==\")\n",
    "    print(f\"Overall: acc={100*overall['acc']:.2f}%, loss={overall['loss']:.3f}\")\n",
    "    print(f\"Forget (airplane): acc={100*forget['acc']:.2f}%, loss={forget['loss']:.3f}\")\n",
    "    print(f\"Retain (non-airplane): acc={100*retain['acc']:.2f}%, loss={retain['loss']:.3f}\")\n",
    "    return RUN_RESULTS[name]\n",
    "\n",
    "def unlearning_distance(candidate_name, full_name=\"FULL\", retrain_name=\"RETRAIN_BASE\", alpha=0.5):\n",
    "    \"\"\"\n",
    "    Compute gaps for an unlearned model vs baselines.\n",
    "    - We want candidate_forget_acc ~ retrain_forget_acc (close to retrain)\n",
    "    - We want candidate_retain_acc ~ full_retain_acc   (close to full)\n",
    "    alpha weighs the forget gap; (1-alpha) weighs the retain gap. Default 0.5/0.5.\n",
    "    Score in [0,1]: higher is better.\n",
    "    \"\"\"\n",
    "    assert candidate_name in RUN_RESULTS, \"Candidate not evaluated yet.\"\n",
    "    assert full_name in RUN_RESULTS and retrain_name in RUN_RESULTS, \"Run FULL and RETRAIN_BASE first (and eval/store).\"\n",
    "    cand   = RUN_RESULTS[candidate_name]\n",
    "    full   = RUN_RESULTS[full_name]\n",
    "    retr   = RUN_RESULTS[retrain_name]\n",
    "\n",
    "    # Accuracies in [0,1]\n",
    "    acc_c_forget = cand[\"forget\"][\"acc\"]\n",
    "    acc_c_retain = cand[\"retain\"][\"acc\"]\n",
    "    acc_f_retain = full[\"retain\"][\"acc\"]\n",
    "    acc_r_forget = retr[\"forget\"][\"acc\"]\n",
    "\n",
    "    # Absolute gaps\n",
    "    forget_gap = abs(acc_c_forget - acc_r_forget)   # want -> 0\n",
    "    retain_gap = abs(acc_c_retain - acc_f_retain)   # want -> 0\n",
    "\n",
    "    # Combined score (normalize by 1 since acc ∈ [0,1])\n",
    "    score = 1.0 - (alpha * forget_gap + (1.0 - alpha) * retain_gap)\n",
    "\n",
    "    return {\n",
    "        \"candidate\": candidate_name,\n",
    "        \"reference_full\": full_name,\n",
    "        \"reference_retrain\": retrain_name,\n",
    "        \"forget_gap\": forget_gap,\n",
    "        \"retain_gap\": retain_gap,\n",
    "        \"alpha\": alpha,\n",
    "        \"score\": max(0.0, min(1.0, score))  # clamp just in case\n",
    "    }\n",
    "\n",
    "def print_unlearning_distance(stats):\n",
    "    print(f\"\\n== Unlearning Distance: {stats['candidate']} ==\")\n",
    "    print(f\"forget_gap (→ retrain): {stats['forget_gap']:.4f}\")\n",
    "    print(f\"retain_gap (→ full)   : {stats['retain_gap']:.4f}\")\n",
    "    print(f\"alpha (forget weight) : {stats['alpha']:.2f}\")\n",
    "    print(f\"UNLEARNING SCORE      : {stats['score']:.4f}  (1.0 is best)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ebb7c1-3ee1-4647-b989-c276724ec5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase A: BASE (train on base classes only)\n",
    "train_model(model_base, loader_base, opt_base, criterion, device, num_epochs=EPOCHS, phase_name=\"BASE\")\n",
    "torch.save({\"model_state\": model_base.state_dict(),\n",
    "            \"optimizer_state\": opt_base.state_dict(),\n",
    "            \"config\": {\"seed\": SEED, \"lr\": LR, \"epochs\": EPOCHS, \"phase\": \"BASE\",\n",
    "                       \"base_classes\": BASE_CLASSES, \"forget_class\": FORGET_CLASS,\n",
    "                       \"per_base\": PER_BASE, \"forget_n\": FORGET_N}}, \"model_base.pt\")\n",
    "eval_and_store(\"BASE\", model_base)\n",
    "\n",
    "# Phase B: FULL (add airplane)\n",
    "train_model(model_full, loader_full, opt_full, criterion, device, num_epochs=EPOCHS, phase_name=\"FULL\")\n",
    "torch.save({\"model_state\": model_full.state_dict(),\n",
    "            \"optimizer_state\": opt_full.state_dict(),\n",
    "            \"config\": {\"seed\": SEED, \"lr\": LR, \"epochs\": EPOCHS, \"phase\": \"FULL\"}}, \"model_full.pt\")\n",
    "eval_and_store(\"FULL\", model_full)\n",
    "\n",
    "# Phase C: RETRAIN_BASE (scratch baseline without airplane)\n",
    "train_model(model_retrain, loader_base, opt_retrain, criterion, device, num_epochs=EPOCHS, phase_name=\"RETRAIN_BASE\")\n",
    "torch.save({\"model_state\": model_retrain.state_dict(),\n",
    "            \"optimizer_state\": opt_retrain.state_dict(),\n",
    "            \"config\": {\"seed\": SEED, \"lr\": LR, \"epochs\": EPOCHS, \"phase\": \"RETRAIN_BASE\"}}, \"model_retrain.pt\")\n",
    "eval_and_store(\"RETRAIN_BASE\", model_retrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc4ceed-2a7f-4fb9-b8c3-7d93d4f0b43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unified unlearning runner,save helpers\n",
    "import os, json\n",
    "from copy import deepcopy\n",
    "from itertools import cycle\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def uniform_kl(logits):\n",
    "    u = torch.full_like(logits, 1.0 / logits.size(1))\n",
    "    return F.kl_div(F.log_softmax(logits, dim=1), u, reduction=\"batchmean\")\n",
    "\n",
    "def ewc_penalty(model, fisher_diag, theta_ref, scale=1.0):\n",
    "    pen = 0.0\n",
    "    if fisher_diag is None or theta_ref is None or scale == 0.0:\n",
    "        return pen\n",
    "    for (n, p) in model.named_parameters():\n",
    "        if p.requires_grad and n in fisher_diag:\n",
    "            diff = (p - theta_ref[n])\n",
    "            pen += (fisher_diag[n] * diff * diff).sum()\n",
    "    return scale * pen\n",
    "\n",
    "# KD masked\n",
    "def kd_loss_masked(student_logits, teacher_logits, mask_class, T=2.0):\n",
    "    tl = teacher_logits.clone()\n",
    "    tl[:, mask_class] = -1e9\n",
    "    t_p    = torch.softmax(tl / T, dim=1)\n",
    "    s_logp = F.log_softmax(student_logits / T, dim=1)\n",
    "    return F.kl_div(s_logp, t_p, reduction=\"batchmean\") * (T*T)\n",
    "\n",
    "def build_variant_name(base=\"GAR\", use_ewc=False, use_kd=False, kd_masked=True, langevin_sigma=0.0):\n",
    "    parts = [base]\n",
    "    if use_ewc: parts.append(\"EWC\")\n",
    "    if use_kd:  parts.append(\"KDm\" if kd_masked else \"KD\")\n",
    "    if langevin_sigma and langevin_sigma > 0:\n",
    "        parts.append(f\"L{langevin_sigma:.0e}\")\n",
    "    return \"_\".join(parts)\n",
    "\n",
    "def save_checkpoint_and_metrics(name, model, metrics, outdir=\"results\"):\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "    # checkpoint\n",
    "    ckpt_path = os.path.join(outdir, f\"model_{name}.pt\")\n",
    "    torch.save({\"state_dict\": model.state_dict(),\n",
    "                \"meta\": {\"variant\": name}}, ckpt_path)\n",
    "    # metrics\n",
    "    with open(os.path.join(outdir, f\"metrics_{name}.json\"), \"w\") as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "    print(f\"[saved] {ckpt_path} and metrics_{name}.json\")\n",
    "\n",
    "def run_unlearn_variant(\n",
    "    name=None,\n",
    "    base_loader=None, forget_loader=None, model_start=None, device=None,\n",
    "    epochs=5, lr=1e-3,\n",
    "    lambda_retain=1.0, lambda_forget=0.8,\n",
    "    use_ewc=False, fisher=None, theta_ref=None, lambda_ewc=0.0,\n",
    "    use_kd=False, lambda_kd=0.0, T_kd=2.0, kd_masked=True,\n",
    "    langevin_sigma=0.0, grad_clip=5.0, print_every=200,\n",
    "    teacher_full=None, forget_class=None\n",
    "):\n",
    "    assert base_loader and forget_loader and model_start is not None\n",
    "    model = deepcopy(model_start).to(device)\n",
    "    model.train()\n",
    "    opt = optim.Adam(model.parameters(), lr=lr)\n",
    "    loop_retain = cycle(base_loader)\n",
    "    loop_forget = cycle(forget_loader)\n",
    "    step = 0\n",
    "\n",
    "    # default name\n",
    "    auto_name = build_variant_name(\"GAR\", use_ewc, use_kd, kd_masked, langevin_sigma)\n",
    "    name = name or auto_name\n",
    "    print(f\"[run] {name}\")\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        steps = min(len(base_loader), len(forget_loader))\n",
    "        for _ in range(steps):\n",
    "            xb, yb = next(loop_retain); xf, yf = next(loop_forget)\n",
    "            xb, yb, xf, yf = xb.to(device), yb.to(device), xf.to(device), yf.to(device)\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            # retain task\n",
    "            logits_b = model(xb)\n",
    "            lb = criterion(logits_b, yb)\n",
    "\n",
    "            # forget task (bounded)\n",
    "            logits_f = model(xf)\n",
    "            lf = uniform_kl(logits_f)\n",
    "\n",
    "            loss = lambda_retain*lb + lambda_forget*lf\n",
    "\n",
    "            # EWC\n",
    "            if use_ewc and lambda_ewc > 0.0:\n",
    "                lewc = ewc_penalty(model, fisher, theta_ref, scale=lambda_ewc)\n",
    "                loss = loss + lewc\n",
    "            else:\n",
    "                lewc = torch.tensor(0.0, device=device)\n",
    "\n",
    "            # KD (retain only)\n",
    "            if use_kd and lambda_kd > 0.0 and teacher_full is not None:\n",
    "                with torch.no_grad():\n",
    "                    t_logits_b = teacher_full(xb)\n",
    "                if kd_masked:\n",
    "                    lkd = kd_loss_masked(logits_b, t_logits_b, mask_class=forget_class, T=T_kd)\n",
    "                else:\n",
    "                    # unmasked KD (not recommended for forgetting)\n",
    "                    t_p    = torch.softmax(t_logits_b / T_kd, dim=1)\n",
    "                    s_logp = F.log_softmax(logits_b / T_kd, dim=1)\n",
    "                    lkd = F.kl_div(s_logp, t_p, reduction=\"batchmean\") * (T_kd*T_kd)\n",
    "                loss = loss + lambda_kd*lkd\n",
    "            else:\n",
    "                lkd = torch.tensor(0.0, device=device)\n",
    "\n",
    "            loss.backward()\n",
    "            if grad_clip is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            if langevin_sigma and langevin_sigma > 0:\n",
    "                for p in model.parameters():\n",
    "                    if p.grad is not None:\n",
    "                        p.grad.add_(langevin_sigma * torch.randn_like(p.grad))\n",
    "            opt.step()\n",
    "\n",
    "            step += 1\n",
    "            if step % print_every == 0:\n",
    "                print(f\"[{name} ep {ep+1}] step {step} \"\n",
    "                      f\"CE={lb.item():.4f} UKL={lf.item():.4f} \"\n",
    "                      f\"EWC={float(lewc):.4f} KD={float(lkd):.4f}\")\n",
    "\n",
    "        print(f\"Epoch {ep+1} done.\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3403635-5493-4429-8bfd-e72a1f273f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable EWC & KD variants\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "# Fisher-information diagonal for EWC\n",
    "def compute_fisher_diag(model, dataloader, device, criterion, n_batches=None):\n",
    "    \"\"\"\n",
    "    Quick empirical Fisher: E[ (∇_θ log p(y|x;θ))² ] over dataloader.\n",
    "    Returns a dict name → tensor (same shape as each param).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    fish = {n: torch.zeros_like(p, device=device) \n",
    "            for n, p in model.named_parameters() if p.requires_grad}\n",
    "    batches_seen = 0\n",
    "\n",
    "    for i, (x, y) in enumerate(dataloader):\n",
    "        if n_batches and i >= n_batches:\n",
    "            break\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        model.zero_grad(set_to_none=True)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y) # NLL <- Cross-Entropy\n",
    "        loss.backward()\n",
    "\n",
    "        for n, p in model.named_parameters():\n",
    "            if p.grad is not None:\n",
    "                fish[n] += p.grad.detach() ** 2\n",
    "        batches_seen += 1\n",
    "\n",
    "    for n in fish:\n",
    "        fish[n] /= max(1, batches_seen) # mean over batches\n",
    "    return fish\n",
    "\n",
    "# (a) reference parameters θ*\n",
    "theta_full_ref  = {n: p.detach().clone() for n, p in model_full.named_parameters() \n",
    "                   if p.requires_grad}\n",
    "# (b) diagonal Fisher on retain/base data, what we want to protect\n",
    "#     (use loader_base so \"airplane\" has no influence)\n",
    "fisher_full_base = compute_fisher_diag(model_full, loader_base, device, criterion, \n",
    "                                       n_batches=100)   #100 mini-batches\n",
    "\n",
    "# Frozen teacher for knowledge distillation\n",
    "teacher_full = deepcopy(model_full).eval().to(device)\n",
    "for p in teacher_full.parameters():\n",
    "    p.requires_grad_(False)\n",
    "\n",
    "print(\"EWC + KD assets ready:\",\n",
    "      f\"|Fisher keys|={len(fisher_full_base)}; teacher params frozen.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3ff6ac-af12-445d-92fe-ff9b28bad491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose variants to run\n",
    "# - EWC needs: theta_full_ref, fisher_full_base\n",
    "# - KD  needs: teacher_full\n",
    "\n",
    "variants = [\n",
    "    # 1) GAR-only (baseline)\n",
    "    dict(name=\"GAR_ONLY\", use_ewc=False, use_kd=False, langevin_sigma=0.0,\n",
    "         lambda_retain=1.0, lambda_forget=0.8),\n",
    "\n",
    "    # 2) GAR + EWC\n",
    "    dict(name=\"GAR_EWC\", use_ewc=True, lambda_ewc=100.0, use_kd=False, langevin_sigma=0.0,\n",
    "         lambda_retain=1.0, lambda_forget=0.8),\n",
    "\n",
    "    # 2b) GAR + EWC + Langevin\n",
    "    # dict(name=\"GAR_EWC_L1e-4\", use_ewc=True, lambda_ewc=100.0, use_kd=False, langevin_sigma=1e-4,\n",
    "    #      lambda_retain=1.0, lambda_forget=0.8),\n",
    "\n",
    "    # 3) GAR + KD masked\n",
    "    # dict(name=\"GAR_KD_m\", use_ewc=False, use_kd=True, kd_masked=True, lambda_kd=0.25, T_kd=2.0,\n",
    "    #      langevin_sigma=0.0, lambda_retain=1.0, lambda_forget=1.0),\n",
    "\n",
    "    # 4) GAR + EWC + KD masked\n",
    "    # dict(name=\"GAR_EWC_KDm_L1e-4\", use_ewc=True, lambda_ewc=100.0, use_kd=True, kd_masked=True,\n",
    "    #      lambda_kd=0.15, T_kd=3.0, langevin_sigma=1e-4, lambda_retain=1.0, lambda_forget=1.0),\n",
    "]\n",
    "\n",
    "# Run and save\n",
    "for cfg in variants:\n",
    "    m = run_unlearn_variant(\n",
    "        name=cfg.get(\"name\"),\n",
    "        base_loader=loader_base, forget_loader=loader_forget, model_start=model_full, device=device,\n",
    "        epochs=EPOCHS, lr=LR,\n",
    "        lambda_retain=cfg.get(\"lambda_retain\", 1.0),\n",
    "        lambda_forget=cfg.get(\"lambda_forget\", 0.8),\n",
    "        use_ewc=cfg.get(\"use_ewc\", False),\n",
    "        fisher=globals().get(\"fisher_full_base\"),\n",
    "        theta_ref=globals().get(\"theta_full_ref\"),\n",
    "        lambda_ewc=cfg.get(\"lambda_ewc\", 0.0),\n",
    "        use_kd=cfg.get(\"use_kd\", False),\n",
    "        lambda_kd=cfg.get(\"lambda_kd\", 0.0),\n",
    "        T_kd=cfg.get(\"T_kd\", 2.0),\n",
    "        kd_masked=cfg.get(\"kd_masked\", True),\n",
    "        langevin_sigma=cfg.get(\"langevin_sigma\", 0.0),\n",
    "        teacher_full=globals().get(\"teacher_full\"),\n",
    "        forget_class=FORGET_CLASS,\n",
    "        grad_clip=5.0, print_every=100\n",
    "    )\n",
    "    eval_and_store(cfg[\"name\"], m)\n",
    "    # save compact metrics snapshot alongside checkpoint\n",
    "    save_checkpoint_and_metrics(cfg[\"name\"], m, RUN_RESULTS[cfg[\"name\"]], outdir=\"results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a63b8c-e844-4145-98a5-a4c91bb7f853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table, CSV (US + DE), and plots\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rows = []\n",
    "order = []\n",
    "\n",
    "# always include FULL & RETRAIN_BASE\n",
    "for base_name in [\"FULL\", \"RETRAIN_BASE\"]:\n",
    "    if base_name in RUN_RESULTS:\n",
    "        r = RUN_RESULTS[base_name]\n",
    "        rows.append({\n",
    "            \"model\": base_name,\n",
    "            \"acc_forget\": r[\"forget\"][\"acc\"],\n",
    "            \"acc_retain\": r[\"retain\"][\"acc\"],\n",
    "            \"acc_overall\": r[\"overall\"][\"acc\"],\n",
    "            \"loss_forget\": r[\"forget\"][\"loss\"],\n",
    "            \"loss_retain\": r[\"retain\"][\"loss\"],\n",
    "            \"loss_overall\": r[\"overall\"][\"loss\"],\n",
    "            \"score\": np.nan\n",
    "        })\n",
    "        order.append(base_name)\n",
    "\n",
    "# add variants and their scores (vs FULL & RETRAIN_BASE)\n",
    "for name, r in RUN_RESULTS.items():\n",
    "    if name in [\"FULL\", \"RETRAIN_BASE\", \"BASE\"]:\n",
    "        continue\n",
    "    s = unlearning_distance(name, full_name=\"FULL\", retrain_name=\"RETRAIN_BASE\", alpha=0.5)\n",
    "    rows.append({\n",
    "        \"model\": name,\n",
    "        \"acc_forget\": r[\"forget\"][\"acc\"],\n",
    "        \"acc_retain\": r[\"retain\"][\"acc\"],\n",
    "        \"acc_overall\": r[\"overall\"][\"acc\"],\n",
    "        \"loss_forget\": r[\"forget\"][\"loss\"],\n",
    "        \"loss_retain\": r[\"retain\"][\"loss\"],\n",
    "        \"loss_overall\": r[\"overall\"][\"loss\"],\n",
    "        \"score\": s[\"score\"]\n",
    "    })\n",
    "    order.append(name)\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# order: FULL, RETRAIN_BASE, then the rest sorted by model name\n",
    "df_sorted = pd.concat([\n",
    "    df[df[\"model\"] == \"FULL\"],\n",
    "    df[df[\"model\"] == \"RETRAIN_BASE\"],\n",
    "    df[~df[\"model\"].isin([\"FULL\",\"RETRAIN_BASE\"])].sort_values(\"model\")\n",
    "], ignore_index=True)\n",
    "\n",
    "# percentage convenience columns (rounded for display)\n",
    "df_sorted[\"acc_forget_pct\"] = (df_sorted[\"acc_forget\"] * 100).round(2)\n",
    "df_sorted[\"acc_retain_pct\"] = (df_sorted[\"acc_retain\"] * 100).round(2)\n",
    "df_sorted[\"score_pct\"] = (df_sorted[\"score\"] * 100).round(2)\n",
    "\n",
    "print(df_sorted.to_string(index=False))\n",
    "\n",
    "# Save CSVs\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "csv_us = \"results/summary.csv\"\n",
    "csv_de = \"results/summary_de.csv\"\n",
    "\n",
    "# US-style CSV (comma sep, dot decimal)\n",
    "df_sorted.to_csv(csv_us, index=False, float_format=\"%.6f\")\n",
    "# DE-style CSV (semicolon sep, comma decimal)\n",
    "df_sorted.to_csv(csv_de, index=False, sep=\";\", float_format=\"%.6f\", decimal=\",\")\n",
    "\n",
    "print(f\"[saved] {csv_us}\")\n",
    "print(f\"[saved] {csv_de}\")\n",
    "\n",
    "# Plots\n",
    "labels = df_sorted[\"model\"].tolist()\n",
    "x = np.arange(len(labels))\n",
    "width = 0.38\n",
    "\n",
    "# 1) Forget vs Retain accuracy (percent)\n",
    "plt.figure(figsize=(9,5))\n",
    "plt.bar(x - width/2, df_sorted[\"acc_forget_pct\"], width, label=\"Forget (airplane)\")\n",
    "plt.bar(x + width/2, df_sorted[\"acc_retain_pct\"], width, label=\"Retain (non-airplane)\")\n",
    "plt.xticks(x, labels, rotation=30, ha=\"right\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.title(\"Unlearning: Forget vs Retain Accuracy\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "acc_plot_path = \"results/acc_bars.png\"\n",
    "plt.savefig(acc_plot_path, dpi=150)\n",
    "plt.show()\n",
    "print(f\"[saved] {acc_plot_path}\")\n",
    "\n",
    "# 2) Unlearning Score (only for variants that have a score)\n",
    "df_score = df_sorted[df_sorted[\"score\"].notna()].copy()\n",
    "if len(df_score) > 0:\n",
    "    x2 = np.arange(len(df_score))\n",
    "    plt.figure(figsize=(7,4.5))\n",
    "    plt.bar(x2, df_score[\"score_pct\"], width=0.5)\n",
    "    plt.xticks(x2, df_score[\"model\"].tolist(), rotation=30, ha=\"right\")\n",
    "    plt.ylabel(\"Unlearning Score (%)\")\n",
    "    plt.title(\"Unlearning Score vs Baselines\")\n",
    "    plt.tight_layout()\n",
    "    score_plot_path = \"results/score_bars.png\"\n",
    "    plt.savefig(score_plot_path, dpi=150)\n",
    "    plt.show()\n",
    "    print(f\"[saved] {score_plot_path}\")\n",
    "else:\n",
    "    print(\"[info] No variant scores to plot.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (unlearn-env)",
   "language": "python",
   "name": "unlearn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
